{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "data_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>discord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ebay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>espn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>walmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>linkedin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nytimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>instructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>duckduckgo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>netflix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tiktok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pinterest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>etsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>zillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>usps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>foxnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>msn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>zoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>roblox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>aol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>fandom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sharepoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>indeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>t-mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>paypal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>twitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>homedepot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>openai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         domain\n",
       "0        google\n",
       "1       youtube\n",
       "2      facebook\n",
       "3        amazon\n",
       "4         yahoo\n",
       "5       twitter\n",
       "6     instagram\n",
       "7           max\n",
       "8     wikipedia\n",
       "9        reddit\n",
       "10      discord\n",
       "11       office\n",
       "12         ebay\n",
       "13         espn\n",
       "14      walmart\n",
       "15     linkedin\n",
       "16      nytimes\n",
       "17  instructure\n",
       "18         bing\n",
       "19         live\n",
       "20   duckduckgo\n",
       "21          cnn\n",
       "22      netflix\n",
       "23        quora\n",
       "24       tiktok\n",
       "25    pinterest\n",
       "26         etsy\n",
       "27      weather\n",
       "28       zillow\n",
       "29         usps\n",
       "30      foxnews\n",
       "31          msn\n",
       "32          zoo\n",
       "33       roblox\n",
       "34          aol\n",
       "35       fandom\n",
       "36       target\n",
       "37   sharepoint\n",
       "38       indeed\n",
       "39     t-mobile\n",
       "40       paypal\n",
       "41       twitch\n",
       "42    microsoft\n",
       "43    homedepot\n",
       "44       openai"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_fname = \"domainsList.csv\"\n",
    "domain_df = pd.read_csv(data_path + domain_fname,names=[\"domain\"])\n",
    "domain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data files\n",
    "data_files = glob.glob(\"./data/run*.json\")\n",
    "data_files.sort() # sort the list so it's processed in order\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for data_file in data_files:\n",
    "    df = pd.read_json(data_file)\n",
    "    \n",
    "    num_cookies = len(df)\n",
    "    num_unique = df[\"domain\"].nunique()\n",
    "\n",
    "    tot_len = total_length = df['value'].apply(len).sum()\n",
    "\n",
    "    filtered_by_domain_df = df[df[\"domain\"].str.contains('|'.join(domain_df['domain']), case=False, regex=True)]\n",
    "    percent_1st = len(filtered_by_domain_df)/len(df)*100\n",
    "    \n",
    "    p_secure = len(df[df[\"secure\"]==True]) / len(df)*100\n",
    "    p_hostonly = len(df[df[\"hostOnly\"]==True]) / len(df)*100\n",
    "    p_httponly = len(df[df[\"httpOnly\"]==True]) / len(df)*100\n",
    "    p_same_site = (len(df[df[\"sameSite\"]==\"no_restriction\"])) / len(df)*100\n",
    "\n",
    "    results = {\"Number of cookies\": num_cookies, \n",
    "               \"Total Length\": tot_len,\n",
    "               \"Number of Unique Domains\": num_unique,\n",
    "                \"% first party\": percent_1st,\n",
    "                \"% secure\": p_secure, \n",
    "                \"% hostOnly\": p_hostonly, \n",
    "                \"% httpOnly\": p_httponly,\n",
    "                \"% sameSite\": p_same_site\n",
    "                }\n",
    "\n",
    "    df_list.append(pd.DataFrame([results]))\n",
    "\n",
    "parsed_info = pd.concat(df_list, ignore_index=True)\n",
    "parsed_info.to_csv(\"ParsedCookieData\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
